; Fiction Fabricator Configuration File
; Lines starting with ; or # are comments.

[API_KEYS]
; Store your API keys in a .env file in the project root for better security.
; The application will automatically load a .env file if it exists.
;
; Example .env file content:
;
; GOOGLE_API_KEY="your_google_api_key"
; MISTRAL_API_KEY="your_mistral_api_key"
; GROQ_API_KEY="your_groq_api_key"
;
; For NVIDIA NIM endpoints, you must provide the API key.
; NVIDIA_API_KEY="nvapi-..."
;
; The Base URL can also be set in the .env file, which will override the
; value in [NVIDIA_SETTINGS] below.
; NVIDIA_BASE_URL="https://your-custom-nim-url.com/v1"
;
; For GitHub Models Marketplace, you must provide your GitHub Access Token
; and the correct endpoint URL.
; GITHUB_ACCESS_TOKEN="github_pat_..."
; AZURE_OPENAI_ENDPOINT="https://models.github.ai/inference"


[LLM_SELECTION]
; Specify the LLM models to be used for different tasks.
; The format is typically: provider://model_identifier
; Examples:
;   google://gemini-1.5-pro-latest
;   mistralai://mistral-large-latest
;   groq://mixtral-8x7b-32768
;   nvidia://meta/llama3-70b-instruct
;   github://o1-mini
;   ollama://llama3:70b
;   ollama://llama3:70b@192.168.1.100:11434 (for Ollama on a specific host)

; Model for generating critiques (used by CritiqueRevision.py)
critique_llm = google://gemini-1.5-flash-latest

; Models for various stages of story generation, matching Writer.Config.py variables
initial_outline_writer_model = google://gemini-1.5-pro-latest
chapter_outline_writer_model = google://gemini-1.5-flash-latest
chapter_stage1_writer_model = google://gemini-1.5-flash-latest
chapter_stage2_writer_model = google://gemini-1.5-flash-latest
chapter_stage3_writer_model = google://gemini-1.5-flash-latest
; Note: Stage 4 is currently commented out in ChapterGenerator
chapter_stage4_writer_model = google://gemini-1.5-flash-latest
chapter_revision_writer_model = google://gemini-1.5-pro-latest
; For generating constructive criticism (LLMEditor)
revision_model = google://gemini-1.5-flash-latest
; For evaluation tasks like rating (LLMEditor)
eval_model = google://gemini-1.5-flash-latest
; For generating summary/info at the end (StoryInfo)
info_model = google://gemini-1.5-flash-latest
; For scrubbing the story (Scrubber)
scrub_model = google://gemini-1.5-flash-latest
; For checking LLM outputs (e.g., summary checks, JSON format)
checker_model = google://gemini-1.5-flash-latest


[NVIDIA_SETTINGS]
; This is a manually curated list. Models for NVIDIA are NOT discovered automatically.
; Add the exact model IDs you have access to here, separated by commas. These will
; appear in the selection menu if your NVIDIA_API_KEY is set.
;
; Example:
; available_moels = meta/llama3-8b-instruct, mistralai/mistral-large

available_models = mistralai/mistral-medium-3-instruct,mistralai/mistral-nemotron,qwen/qwen3-235b-a22b,nvidia/llama-3.1-nemotron-ultra-253b-v1,nvidia/llama-3.3-nemotron-super-49b-v1,writer/palmyra-creative-122b
; The base URL for the NVIDIA API. The default is for NVIDIA's managed endpoints.
; This can be overridden by setting NVIDIA_BASE_URL in your .env file.
base_url = https://integrate.api.nvidia.com/v1


[GITHUB_SETTINGS]
; API Version required by the Azure OpenAI client used for the GitHub provider.
api_version = 2024-05-01-preview


[WRITER_SETTINGS]
; Seed for randomization in LLM generation. Overridden by command-line -Seed.
seed = 108

; Outline generation revision settings. Overridden by command-line args.
outline_min_revisions = 0
outline_max_revisions = 3

; Chapter generation revision settings. Overridden by command-line args.
; Valid values: true, false, yes, no, on, off, 1, 0
chapter_no_revisions = false
chapter_min_revisions = 1
chapter_max_revisions = 3
minimum_chapters = 12

; Other generation settings. Overridden by command-line args.
; Disables final scrub pass.
scrub_no_scrub = false
; Enables chapter-by-chapter outline expansion.
expand_outline = true
; Enables a full-novel edit pass before scrubbing.
enable_final_edit_pass = false
; Uses scene-by-scene generation.
scene_generation_pipeline = true

; Debug mode. Overridden by command-line -Debug.
debug = false

; Ollama specific settings (if Ollama is use)
ollama_ctx = 8192

[PROJECT_INFO]
project_name = Fiction Fabricator

[TIMEOUTS]
; Request timeout in seconds. It's recommended to have a longer timeout
; for local providers like Ollama that may have long load times.
default_timeout = 180
ollama_timeout = 360
